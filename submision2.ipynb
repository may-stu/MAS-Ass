{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca98f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# === Hyperparameters ===\n",
    "GRID_SIZE = 5\n",
    "NUM_AGENTS = 4\n",
    "ALPHA = 1e-3            # learning rate\n",
    "GAMMA = 0.95           # discount factor\n",
    "EPSILON_START = 1.0    # initial exploration rate\n",
    "EPSILON_MIN = 0.1      # minimum exploration rate (higher for sustained exploration)\n",
    "EPSILON_DECAY = 0.9999 # slower decay for prolonged exploration\n",
    "MAX_EPISODE_STEPS = 25 # match evaluation horizon\n",
    "STEP_BUDGET = 1_500_000  # total agent-steps budget\n",
    "\n",
    "# === Reward weights ===\n",
    "REWARD_STEP      = -0.01  # per-step penalty\n",
    "REWARD_PICKUP    = +2.0  # pickup reward\n",
    "REWARD_DELIVERY  = +7.0  # delivery reward (amplified)\n",
    "REWARD_COLLISION = -15.0 # collision penalty\n",
    "SHAPING_COEFF    = 0.2   # potential-based shaping coefficient\n",
    "\n",
    "# === Training flags ===\n",
    "USE_SENSOR = True      # disable sensor to reduce state-space initially\n",
    "USE_OFFJOB_TRAINING = True\n",
    "USE_CENTRAL_CLOCK = True\n",
    "\n",
    "# Precompute all (A,B) pairs for curriculum sampling\n",
    "ALL_PAIRS = [((x, y), (u, v))\n",
    "             for x in range(GRID_SIZE) for y in range(GRID_SIZE)\n",
    "             for u in range(GRID_SIZE) for v in range(GRID_SIZE)\n",
    "             if (x, y) != (u, v)]\n",
    "\n",
    "coverage = {pair: 0 for pair in ALL_PAIRS}\n",
    "\n",
    "def randomize_locations():\n",
    "    \"\"\"\n",
    "    Sample (A,B) inversely proportional to coverage for curriculum.\n",
    "    \"\"\"\n",
    "    weights = [1.0 / (coverage[pair] + 1)**2 for pair in ALL_PAIRS]\n",
    "    pair = random.choices(ALL_PAIRS, weights=weights, k=1)[0]\n",
    "    coverage[pair] += 1\n",
    "    return pair\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, agent_id, shared_q):\n",
    "        self.id = agent_id\n",
    "        self.q_table = shared_q\n",
    "        self.epsilon = EPSILON_START\n",
    "        self.reset(None, False)\n",
    "\n",
    "    def reset(self, start_pos, carrying):\n",
    "        self.pos = start_pos\n",
    "        self.carrying = carrying\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "    def get_state(self, grid, A_loc, B_loc):\n",
    "        x, y = self.pos\n",
    "        c = int(self.carrying)\n",
    "        mask = 0\n",
    "        if USE_SENSOR:\n",
    "            dirs = [(-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1)]\n",
    "            for i, (dx,dy) in enumerate(dirs):\n",
    "                nx, ny = x+dx, y+dy\n",
    "                if 0 <= nx < GRID_SIZE and 0 <= ny < GRID_SIZE:\n",
    "                    cell = grid[nx][ny]\n",
    "                    if cell:\n",
    "                        for occ_id, occ_carry in cell:\n",
    "                            if occ_id != self.id and occ_carry != c:\n",
    "                                mask |= (1 << i)\n",
    "                                break\n",
    "        dxA, dyA = A_loc[0] - x, A_loc[1] - y\n",
    "        dxB, dyB = B_loc[0] - x, B_loc[1] - y\n",
    "        return (x, y, c, mask, dxA, dyA, dxB, dyB)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(4)\n",
    "        q = self.q_table.setdefault(state, np.zeros(4))\n",
    "        return int(np.argmax(q))\n",
    "\n",
    "    def update_q(self, state, action, reward, next_state):\n",
    "        q = self.q_table.setdefault(state, np.zeros(4))\n",
    "        q_next = self.q_table.setdefault(next_state, np.zeros(4))\n",
    "        q[action] += ALPHA * (reward + GAMMA * np.max(q_next) - q[action])\n",
    "\n",
    "\n",
    "def step_all_agents(agents, grid, A_loc, B_loc, train=True):\n",
    "    proposals = []\n",
    "    for agent in agents:\n",
    "        # record old distance for shaping\n",
    "        if agent.carrying:\n",
    "            old_dist = abs(agent.pos[0]-B_loc[0]) + abs(agent.pos[1]-B_loc[1])\n",
    "        else:\n",
    "            old_dist = abs(agent.pos[0]-A_loc[0]) + abs(agent.pos[1]-A_loc[1])\n",
    "        state = agent.get_state(grid, A_loc, B_loc)\n",
    "        action = agent.choose_action(state)\n",
    "        dx, dy = [(-1,0),(1,0),(0,-1),(0,1)][action]\n",
    "        nx, ny = agent.pos[0] + dx, agent.pos[1] + dy\n",
    "        if not (0 <= nx < GRID_SIZE and 0 <= ny < GRID_SIZE):\n",
    "            nx, ny = agent.pos\n",
    "        proposals.append((agent, agent.pos, (nx, ny), action, state, old_dist, agent.carrying))\n",
    "\n",
    "    # detect head-on collisions\n",
    "    collisions = set()\n",
    "    for i, (ai, old_i, new_i, _, _, _, _) in enumerate(proposals):\n",
    "        for j, (aj, old_j, new_j, _, _, _, _) in enumerate(proposals):\n",
    "            if i < j and old_i == new_j and old_j == new_i and ai.carrying != aj.carrying:\n",
    "                if old_i not in [A_loc, B_loc] and old_j not in [A_loc, B_loc]:\n",
    "                    collisions.add((ai.id, aj.id))\n",
    "\n",
    "    new_grid = [[[] for _ in range(GRID_SIZE)] for _ in range(GRID_SIZE)]\n",
    "    total_reward = 0.0\n",
    "    for agent, old_pos, new_pos, action, state, old_dist, carried_before in proposals:\n",
    "        # per-step + shaping\n",
    "        reward = REWARD_STEP\n",
    "        if carried_before:\n",
    "            new_dist = abs(new_pos[0]-B_loc[0]) + abs(new_pos[1]-B_loc[1])\n",
    "        else:\n",
    "            new_dist = abs(new_pos[0]-A_loc[0]) + abs(new_pos[1]-A_loc[1])\n",
    "        reward += SHAPING_COEFF * (old_dist - new_dist)\n",
    "        # collision penalty\n",
    "        if any(agent.id in pair for pair in collisions):\n",
    "            reward += REWARD_COLLISION\n",
    "        agent.pos = new_pos\n",
    "        # pickup/delivery\n",
    "        if not agent.carrying and agent.pos == A_loc:\n",
    "            agent.carrying = True\n",
    "            reward += REWARD_PICKUP\n",
    "        elif agent.carrying and agent.pos == B_loc:\n",
    "            agent.carrying = False\n",
    "            reward += REWARD_DELIVERY\n",
    "        new_grid[agent.pos[0]][agent.pos[1]].append((agent.id, agent.carrying))\n",
    "        next_state = agent.get_state(new_grid, A_loc, B_loc)\n",
    "        if train:\n",
    "            agent.update_q(state, action, reward, next_state)\n",
    "        total_reward += reward\n",
    "    return new_grid, len(collisions), total_reward\n",
    "\n",
    "\n",
    "import time, random, pickle\n",
    "\n",
    "def train(num_episodes=10000, collision_budget=3999, time_budget=600):\n",
    "    shared_q = {}\n",
    "    agents = [Agent(i, shared_q) for i in range(NUM_AGENTS)]\n",
    "    total_collisions = 0\n",
    "    total_steps      = 0\n",
    "    start_time = time.time()\n",
    "    coverage = {}\n",
    "\n",
    "    for ep in range(1, num_episodes+1):\n",
    "        ep_start_time = time.time()\n",
    "        ep_collisions = 0\n",
    "        ep_steps      = 0\n",
    "\n",
    "        # 1) sample (A,B) and track coverage\n",
    "        A_loc, B_loc = randomize_locations()\n",
    "        coverage[(A_loc, B_loc)] = coverage.get((A_loc, B_loc), 0) + 1\n",
    "\n",
    "        # 2) reset grid\n",
    "        grid = [[[] for _ in range(GRID_SIZE)] for _ in range(GRID_SIZE)]\n",
    "\n",
    "        # 3) pick start positions\n",
    "        if USE_OFFJOB_TRAINING:\n",
    "            starts = [random.choice([A_loc, B_loc]) for _ in range(NUM_AGENTS)]\n",
    "        else:\n",
    "            starts = [A_loc] * (NUM_AGENTS//2) + [B_loc] * (NUM_AGENTS - NUM_AGENTS//2)\n",
    "            random.shuffle(starts)\n",
    "\n",
    "        # 4) reset each agent so that agent.pos is never None\n",
    "        for agent, start in zip(agents, starts):\n",
    "            initial_carry = (start == A_loc)\n",
    "            agent.reset(start, initial_carry)\n",
    "            grid[start[0]][start[1]].append((agent.id, agent.carrying))\n",
    "\n",
    "        # --- LOGGING: Print episode and agent starts like evaluation ---\n",
    "        print(f\"\\n[Train] Episode {ep}: A={A_loc}, B={B_loc}\")\n",
    "        for agent, start in zip(agents, starts):\n",
    "            loc_str = \"A_loc\" if start == A_loc else \"B_loc\"\n",
    "            print(f\"    Agent {agent.id} starts at {loc_str} ({start})\")\n",
    "        # -------------------------------------------------------------\n",
    "\n",
    "        # --- Advanced per-agent cycle step tracking (A→B→A or B→A→B) ---\n",
    "        cycle_start = {ag.id: None for ag in agents}\n",
    "        visited_other = {ag.id: False for ag in agents}\n",
    "        completion_step = {ag.id: None for ag in agents}\n",
    "\n",
    "        for step in range(1, MAX_EPISODE_STEPS + 1):\n",
    "            ep_steps += 1\n",
    "            order = agents if USE_CENTRAL_CLOCK else random.sample(agents, len(agents))\n",
    "            grid, collisions, *_ = step_all_agents(order, grid, A_loc, B_loc)\n",
    "            ep_collisions += collisions\n",
    "            total_collisions += collisions  # <-- ADD THIS LINE\n",
    "\n",
    "            for ag, start in zip(agents, starts):\n",
    "                # Mark the start of the cycle\n",
    "                if cycle_start[ag.id] is None:\n",
    "                    cycle_start[ag.id] = step\n",
    "                # Determine the \"other\" location\n",
    "                other_loc = B_loc if start == A_loc else A_loc\n",
    "                # Mark if agent has visited the other location\n",
    "                if not visited_other[ag.id] and ag.pos == other_loc:\n",
    "                    visited_other[ag.id] = True\n",
    "                # If agent has returned to start after visiting other, cycle is complete\n",
    "                if (visited_other[ag.id] and ag.pos == start and\n",
    "                    completion_step[ag.id] is None and step > cycle_start[ag.id]):\n",
    "                    completion_step[ag.id] = step - cycle_start[ag.id] + 1\n",
    "\n",
    "            # If all agents have completed a cycle, stop early\n",
    "            if all(completion_step[ag.id] is not None for ag in agents):\n",
    "                break\n",
    "        # -------------------------------------------------------------\n",
    "\n",
    "        # Log per-agent cycle steps\n",
    "        for ag in agents:\n",
    "            s = completion_step[ag.id]\n",
    "            if s is not None and s <= MAX_EPISODE_STEPS:\n",
    "                print(f\"  Agent {ag.id} completed cycle in {s} steps\")\n",
    "            elif s is not None:\n",
    "                print(f\"  Agent {ag.id} completed cycle in {s} steps (exceeds {MAX_EPISODE_STEPS})\")\n",
    "            else:\n",
    "                print(f\"  Agent {ag.id} did not complete a full cycle in {MAX_EPISODE_STEPS} steps\")\n",
    "\n",
    "        total_steps += ep_steps\n",
    "\n",
    "        # 6) decay epsilon\n",
    "        for agent in agents:\n",
    "            agent.epsilon = max(EPSILON_MIN, agent.epsilon * EPSILON_DECAY)\n",
    "\n",
    "        # 7) log this episode\n",
    "        ep_time = time.time() - ep_start_time\n",
    "        print(f\"[Train] Ep {ep:5d} → steps: {ep_steps:3d}, \"\n",
    "              f\"collisions: {ep_collisions:2d}, time: {ep_time:5.2f}s\")\n",
    "\n",
    "        # 8) optional exploration burst\n",
    "        if ep % 10000 == 0 and ep <= num_episodes - 10000:\n",
    "            for agent in agents:\n",
    "                agent.epsilon = EPSILON_START\n",
    "            print(f\"*** Exploration burst at ep {ep} ***\")\n",
    "\n",
    "        if total_collisions > collision_budget:\n",
    "            print(f\"Collision budget exceeded at episode {ep}. Stopping training.\")\n",
    "            break\n",
    "        if (time.time() - start_time) > time_budget:\n",
    "            print(f\"Time budget exceeded at episode {ep}. Stopping training.\")\n",
    "            break\n",
    "\n",
    "    # end episodes loop\n",
    "\n",
    "    # report least-trained pairs\n",
    "    least = sorted(coverage.items(), key=lambda kv: kv[1])[:5]\n",
    "    print(\"Least-trained (A,B) pairs and counts:\", least)\n",
    "\n",
    "    # save Q-table\n",
    "    with open(\"q_table.pkl\", \"wb\") as f:\n",
    "        pickle.dump(shared_q, f)\n",
    "\n",
    "    # final summary\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTraining complete.\")\n",
    "    print(f\"  Total episodes run : {ep}\")\n",
    "    print(f\"  Total agent-steps  : {total_steps}\")\n",
    "    print(f\"  Total collisions   : {total_collisions}\")\n",
    "    print(f\"  Total wall-time    : {total_time:.2f} seconds\")\n",
    "\n",
    "    return shared_q\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(q_table, scenarios=None, max_steps=25):\n",
    "    if scenarios is None:\n",
    "        scenarios = ALL_PAIRS\n",
    "    total = len(scenarios)\n",
    "    successes = 0\n",
    "    print(\"Starting evaluation with per-episode logging...\")\n",
    "\n",
    "    for idx, (A_loc, B_loc) in enumerate(scenarios, start=1):\n",
    "        print(f\"\\nEpisode {idx}/{total}: A={A_loc}, B={B_loc}\")\n",
    "        grid = [[[] for _ in range(GRID_SIZE)] for _ in range(GRID_SIZE)]\n",
    "        agents = [Agent(i, q_table) for i in range(NUM_AGENTS)]\n",
    "        agent_starts = [random.choice([A_loc, B_loc]) for _ in range(NUM_AGENTS)]\n",
    "        for agent, start in zip(agents, agent_starts):\n",
    "            initial_carry = (start == A_loc)\n",
    "            agent.reset(start, initial_carry)\n",
    "            agent.epsilon = 0.0\n",
    "            grid[start[0]][start[1]].append((agent.id, agent.carrying))\n",
    "\n",
    "        # Log agent starting positions\n",
    "        for agent, start in zip(agents, agent_starts):\n",
    "            loc_str = \"A_loc\" if start == A_loc else \"B_loc\"\n",
    "            print(f\"    Agent {agent.id} starts at {loc_str} ({start})\")\n",
    "\n",
    "        # For each agent, track:\n",
    "        # - If they started at A, when they first reach B (cycle_start)\n",
    "        # - When they complete B→A→B (completion_step)\n",
    "        picked_up = {ag.id: False for ag in agents}\n",
    "        cycle_start = {ag.id: None for ag in agents}\n",
    "        completion_step = {ag.id: None for ag in agents}\n",
    "        collision_at = None\n",
    "\n",
    "        for step in range(1, max_steps * 2 + 1):  # Allow up to 2x steps for agents starting at A\n",
    "            grid, collisions, _ = step_all_agents(agents, grid, A_loc, B_loc, train=False)\n",
    "            if collisions > 0:\n",
    "                collision_at = step\n",
    "                break\n",
    "            for ag, start in zip(agents, agent_starts):\n",
    "                # If agent started at A and reaches B for the first time, mark cycle_start\n",
    "                if start == A_loc and cycle_start[ag.id] is None and ag.pos == B_loc:\n",
    "                    cycle_start[ag.id] = step\n",
    "                    picked_up[ag.id] = False  # Reset pickup for the cycle\n",
    "                # If agent started at B, cycle_start is 1\n",
    "                if start == B_loc:\n",
    "                    cycle_start[ag.id] = 1\n",
    "                # Only count pickup/delivery after cycle_start\n",
    "                if cycle_start[ag.id] is not None and step >= cycle_start[ag.id]:\n",
    "                    # Pickup at A\n",
    "                    if not picked_up[ag.id] and ag.pos == A_loc and ag.carrying:\n",
    "                        picked_up[ag.id] = True\n",
    "                    # Delivery at B after pickup\n",
    "                    if picked_up[ag.id] and not ag.carrying and ag.pos == B_loc and completion_step[ag.id] is None:\n",
    "                        completion_step[ag.id] = step - cycle_start[ag.id] + 1  # Steps taken in cycle\n",
    "            # If all agents have completed the cycle, stop\n",
    "            if all(completion_step[ag.id] is not None for ag in agents):\n",
    "                break\n",
    "\n",
    "        for ag in agents:\n",
    "            s = completion_step[ag.id]\n",
    "            if collision_at is not None:\n",
    "                print(f\"  Agent {ag.id} did not finish due to collision at step {collision_at}\")\n",
    "            elif s is not None and s <= max_steps:\n",
    "                print(f\"  Agent {ag.id} succeeded in B→A→B cycle in {s} steps\")\n",
    "            elif s is not None:\n",
    "                print(f\"  Agent {ag.id} completed cycle in {s} steps (exceeds {max_steps})\")\n",
    "            else:\n",
    "                print(f\"  Agent {ag.id} failed to complete B→A→B cycle within {max_steps} steps\")\n",
    "\n",
    "        episode_success = (\n",
    "            collision_at is None and\n",
    "            all(completion_step[ag.id] is not None and completion_step[ag.id] <= max_steps for ag in agents)\n",
    "        )\n",
    "        status = \"SUCCESS\" if episode_success else \"FAILURE\"\n",
    "        print(f\"Episode {idx} Result: {status}\")\n",
    "        if episode_success:\n",
    "            successes += 1\n",
    "\n",
    "    rate = 100 * successes / total\n",
    "    print(f\"\\nOverall success: {successes}/{total} episodes → {rate:.2f}%\")\n",
    "    return rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bddb8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = train()\n",
    "final_rate = evaluate(q_table)\n",
    "print(f\"Final evaluation success rate: {final_rate:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
